---
title: An Ensemble Approach to Predicting Drug-induced Liver Injury based on RNA Expression Levels
author: Rex Sumsion
date: '2018-05-07'
slug: camda-cmap-drug-safety-challenge
categories: [Machine Learning, Python, Data Science, Image Recognition, Support Vector Machine, Multilayer Perceptron, Random Forest, KNearest Neighbor, Naive Bayes, Logistic Regression, Gradient Boosting, Soft Voting Method]
tags:
  - Machine Learning
  - Python
  - Data Science
  - Image Recognition
  - Support Vector Machine
  - Multilayer Perceptron
  - Random Forest
  - KNearest Neighbor
  - Naive Bayes
  - Logistic Regression
  - Gradient Boosting
  - Soft Voting Method
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
type: "post"
---

G. Rex Sumsion1, Michael S. Bradshaw III1, Jeremy T. Beales1, Emi Ford1, Griffin R. G. Caryotakis1, Daniel J. Garrett1, Emily D. LeBaron1, Ifeanyichukwu O. Nwosu1, and Stephen R. Piccolo1,*

1Brigham Young University, Department of Biology, Provo, UT 84602 (USA)

* Please address correspondence to Stephen R. Piccolo, stephen_piccolo@byu.edu.



## Introduction

  Drug-induced liver injury (DILI) remains a serious concern during drug development. Evidence suggests that reactive drug metabolites play a role initiating DILI1. DILI is characterized by elevated levels of alanine aminotransferase, and it can ultimately result in patient death in serious cases. To quantify the effects of drugs on humans, the Connectivity Map (CMap, build 02) data set measures drugs’ impact on RNA expression in cancer cell lines. In this paper we outline an attempt to use RNA expression data before and after drug exposure to predict whether specific drugs in CMap cause hepatic injury. First, we applied seven classification algorithms independently. None of these algorithms predicted liver injury on a consistent basis with high accuracy. In an attempt to improve accuracy, we aggregated predictions for six of the algorithms (excluding one that had performed exceptionally poorly) using a soft-voting ensemble method. This approach improved our results in some cases for the training set but failed to generalize well to the test set. We conclude that  more robust methods and/or datasets will be necessary to effectively predict drug-induced liver injury based on RNA expression levels in cell lines.
  
  

## Methods

### Data pre-processing

  We used the SCAN algorithm2 to normalize the individual CEL files, which we downloaded from the Connectivity Map website (https://portals.broadinstitute.org/cmap/). Afterward, we used BrainArray annotations to summarize the values at the gene level (using Entrez Gene identifiers)3. We wrote custom Python scripts to rename the output files based on Connectivity Map metadata. The scripts we used to normalize and prepare the data can be found here: https://osf.io/v3qyg/.

For each independent cell line, we averaged gene expression values across the multiple vehicle files. We then subtracted these values from the corresponding gene-expression values for the compounds of interest. We then merged these data into separate files for MCF7 and PC3, respectively.
	
### Feature selection and cross-validation

  Initially, we used a variance-based, feature-selection approach. However, in performing 10-fold cross validation on the training set, we observed that this approach reduced accuracy, so we excluded it from our final solution. To perform cross-validation, we wrote custom Python code that utilizes the scikit-learn module.

### Classification

  We performed classification using algorithms from the scikit-learn library (version 0.19.1) for the Python programing language4. We assessed the performance of the following algorithms: Gradient Boosting5, Logistic Regression6, K-nearest Neighbor7, Random Forest8, Multilayer Perceptron9, Support Vector Machines10, and Gaussian Naïve Bayes11. 

  For our ensemble classifier, we used the VotingClassifier class in scikit-learn. The ensemble classifier used soft voting, predicting the class label based on the average of the predictions across individual algorithms12. 
  
  
  
## Results

  Initially, we sought to select optimal parameters for each individual algorithm. We evaluated a variety of parameter combinations for each algorithm, assessing each combination based on its effect on accuracy, specificity, sensitivity, and MCC. For example, in evaluating the “number of trees” hyperparameter for the Random Forest algorithm, we used values ranging between 5 and 100 and found that all 4 metrics performed relatively well at 25 trees (Figure 1). We used a similar approach to optimize additional hyperparameters (tree depth, maximum leaf nodes, minimum number of samples required to split an internal node, minimum samples per leaf) in this same fashion. This approach did not consider possible interactions across hyperparameters; however, it enabled us to visually examine the effects of many parameters. Table 1 is a summary of the parameters used in our final solutions (default values were used unless otherwise noted). The performances of the individual optimized algorithms are also included in Table 1.

  In an attempt to optimize the performance of the ensemble classifier, we assessed several variations of the individual algorithms, at times assigning weights to these algorithms based on their predictive ability and excluding algorithms with weaker performances. The only approach that appeared to have a consistently positive effect on performance was to exclude the Gaussian Naïve Bayes algorithm, which had performed quite poorly in isolation (Table 1). Consequently, we decided to include all individual algorithms (exception Gaussian Naïve Bayes) in our final ensemble classifier and refrained from using weights.

  For our final CAMDA submission, we selected two algorithms (Logistic Regression and Random Forest) to submit independently, in addition to our ensemble method. Final results for these three methods on the test set are shown in Figure 2. We submitted these three methods to the CAMDA competition in the following order: ensemble method, Random Forest, and Logistic Regression. Although the ensemble method resulted in slightly lower performance than the best individual algorithms on the training data, we anticipated that it would yield the best results on the test data because it would be robust to poor performance of individual algorithms while benefiting from the diversity of predictions for the relatively accurate algorithms.

  None of our solutions produced consistently accurate predictions on the test set (Figure 2). Accuracy for the ensemble classifier increased relative to its accuracy on the training dataset?, but it was well below the baseline accuracy (assuming the majority class were predicted by default). A large majority of the test samples were predicted to result in liver injury, resulting in higher sensitivity but lower specificity (Figures 2B-C), likely due to class imbalance.
  
  
  
## Discussion

  Using an ensemble method that aggregated predictions from six different classifiers sometimes yielded better performance than the individual algorithms, but this pattern was not consistent. On the test set, the Random Forest and Logistic Regression algorithms outperformed our ensemble approach in some cases. However, none of our methods produced a highly accurate classifier when considering all the metrics. 

  Even though the levels of accuracy of our classifiers were higher in the test data than the training data, these values are below baseline expectations due to an increase in class imbalance in the test set. We believe this factor was a key reason that our methods underperformed. It may have been valuable to employ methods that more explicitly account for class imbalance, such as data oversampling, assigning higher weights to the minority class, etc.13.
  
  
  
## Acknowledgements

  Participating in this competition provided invaluable educational experiences for the authors of this study, most of whom were members of the Bioinformatics Research Group (BRG) at Brigham Young University. The BRG almost exclusively consists of and is led by undergraduate students. The BRG has three goals: to help students develop research skills, to actively publish research, and to provide students with additional exposure to bioinformatics applications in industry. The BRG meets weekly to pursue these goals. For this competition, we assigned a project leader (GRS) who oversaw the project and mentored less experienced students. We thank the CAMDA organizers for providing the opportunity to participate in this competition and gain experience in this exciting research area. 
  
  

## References

1. Tailor, A., Faulkner, L., Naisbitt, D. J. & Park, B. K. The chemical, genetic and immunological basis of idiosyncratic drug-induced liver injury. Hum. Exp. Toxicol. 34, 1310-1317 (2015).
2. Piccolo, S. R. et al. A single-sample microarray normalization method to facilitate personalized-medicine workflows. Genomics 100, 337-344 (2012).
3. Dai, M. et al. Evolving gene/transcript definitions significantly alter the interpretation of GeneChip data. Nucleic Acids Res. 33, e175 (2005).
4.Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825-2830 (2011). 
5. Friedman, J. H. Greedy Function Approximation: A Gradient Boosting Machine. Ann. Stat. 29, 1189-1232 (2001).
6. Yu, H., Huang, F. & Lin, C. Dual coordinate descent methods for logistic regression and maximum entropy models. Mach. Learn. 85, 41-75 (2011).
7. Altman, N. S. An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression. Am. Stat. 46, 175-185 (1992).
8. Breiman, L. Random Forests. Mach. Learn. 45, 5-32 (2001).
9. Mühlenbein, H. Limitations of multi-layer perceptron networks - steps towards genetic neural networks. Parallel Comput. 14, 249-260 (1990).
10. Decoste, D. & Schölkopf, B. Training Invariant Support Vector Machines. Mach. Learn. 46, 161-190 (2002).
11. Hand, D. J. & Yu, K. Idiot's Bayes: Not So Stupid after All? International Statistical Review / Revue Internationale de Statistique 69, 385-398 (2001).
12. Opitz, D. & Maclin, R. Popular Ensemble Methods: An Empirical Study. J. ARTIF. INTELL. RES. 11, 169-198 (1999).
13. Longadge, R., Dongre, S. & Malik, L. Class Imbalance Problem in Data Mining: Review. International Journal of Computer Science and Network 2 (2013).



## Figures and Tables


![](/post/2018-05-07-camda-cmap-drug-safety-challenge_files/optomization.png)

Figure 1: Results of hyperparameter optimization based on the "number of trees" hyperparameter for the Random Forest algorithm. All 4 metrics peaked around 25 trees (red dashed lines).




|           | Accuracy-PC3 | Accuracy-MCF7 | Sensitivity-PC3 | Sensitivity-MCF7 | Specificity-PC3 | Specificity-MCF7 | MCC-PC3 | MCC-MCf7 |
|------------|-----|------|-----|------|-----|------|-----|----|
| sklearn.neural_network.MLPClassifier hidden_layer_sizes=(30,30,30,30,30,30,30,30,30,30), learning_rate_init = 0.0376 | 0.63 | 0.65 | 0.69 | 0.69 | 0.32 | 0.35 | 0.01 | 0.03 |
| sklearn.ensemble.GradientBoostingClassifier learning_rate = 0.31, max_depth = 3 | 0.67 | 0.60 | 0.69 | 0.67 | 0.39 | 0.27 | 0.04 | -0.05 | 
| sklearn.neighbors.KNeighborsClassifier n_neighbors=8, weights='distance' | 0.68 | 0.64 | 0.70 | 0.72 | 0.50 | 0.41 | 0.11 | 0.12 |
| sklearn.linear_model.LogisticRegression solver='lbfgs' | 0.70 | 0.62 | 0.72 | 0.68 | 0.57 | 0.27 | 0.20 | -0.04 |
| sklearn.naive_bayes.GaussianNB | 0.35 | 0.35 | 0.71 | 0.73 | 0.32 | 0.32 | 0.02 | 0.03 | 
| sklearn.ensemble.RandomForestClassifier n_estimators=25, max_depth=9, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0, max_leaf_nodes=25, bootstrap=False, random_state=0 | 0.66 | 0.70 | 0.69 | 0.72 | 0.33 | 0.54 | 0.01 | 0.19 |
| sklearn.svm.SVC | 0.68 | 0.68 | 0.68 | 0.68 | nan | nan | nan | nan | 
| sklearn.ensemble.VotingClassifier voting='soft' | 0.68 | 0.67 | 0.69 | 0.69 | 0.44 | 0.33 | 0.06 | 0.01 |


Table 1: Performance metrics for the individual classifiers (after hyperparameter optimization) and our ensemble method on the training set.



![](/post/2018-05-07-camda-cmap-drug-safety-challenge_files/graphs.png)

Figure 2: Training and test results of our three submitted classification methods. A) shows the classification accuracy compared to baseline expectations. B-D show sensitivity, specificity, and the Matthews Correlation Coefficient (MCC) for the same.

 



